{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a192000",
   "metadata": {},
   "source": [
    "## Question answering based on the OpenAI cookbook tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6a296",
   "metadata": {},
   "source": [
    "OpenAI has a very thorough tutorial on creating a question answering function using embeddings (https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb).\n",
    "One of the possible solutions was to follow this tutorial.\n",
    "\n",
    "The steps modified from the tutorial are: \n",
    "- Prepare the laws data\n",
    "- Create embeddings for the data\n",
    "- Create embedding for the query\n",
    "- Find most relevant text sections (by calculating distance between the query embedding and text embeddings)\n",
    "- Send query with the question and most relevant sections\n",
    "- Get query answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3e8a9",
   "metadata": {},
   "source": [
    "## Creating the embeddings file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef3a4d",
   "metadata": {},
   "source": [
    "A little section about what embeddings are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e31195",
   "metadata": {},
   "source": [
    "### Overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bfe75",
   "metadata": {},
   "source": [
    "The original dataset contains:\n",
    "- the type or title of the law, for example VVS is \"Vabariigi Valitsuse seadus\" which translates to Government of the Republic Act\n",
    "- the paragraph number\n",
    "- the text of the paragraph\n",
    "- the link to the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d5e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from fastparquet import write\n",
    "from tables import *\n",
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  \n",
    "import tiktoken  # for counting tokens\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "from vector_database import save_index\n",
    "from vector_database import load_index\n",
    "from vector_database import strings_ranked_by_relatedness_vector\n",
    "from answer_rater import rank_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486c3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use embeddings you must create an .env file where the content is OPENAI_API_KEY = \"your-api-key\"\n",
    "config = dotenv_values(\".env\")[\"OPENAI_API_KEY\"]\n",
    "openai.organization = \"org-3O7bHGD9SwjHVDuUCNCGACC3\"\n",
    "openai.api_key = config\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94e28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"legal_acts_estonia.csv\", names=['type', 'nr','text','link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3689441b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>nr</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VVS</td>\n",
       "      <td>para1</td>\n",
       "      <td>§ 1.\\nVabariigi Valitsuse pädevus\\n(1) Vabarii...</td>\n",
       "      <td>https://www.riigiteataja.ee/akt/VVS#para1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type     nr                                               text  \\\n",
       "0  VVS  para1  § 1.\\nVabariigi Valitsuse pädevus\\n(1) Vabarii...   \n",
       "\n",
       "                                        link  \n",
       "0  https://www.riigiteataja.ee/akt/VVS#para1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fafb9",
   "metadata": {},
   "source": [
    "### Cleaning and splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301fcf6d",
   "metadata": {},
   "source": [
    "The tutorial suggests that long sections, which have over 1600 tokens should be split down to smaller sections. Splitting the sections allows the question query to match smaller and more  specific sections to the query and add them to the prompt, without exceeding the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94f50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd10124",
   "metadata": {},
   "source": [
    "When splitting the paragraphs, it is important to still have the context of the paragraph, which is why the title of the paragraph is split from the text and later added to the subsections of the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeadfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"text\"].str.split(\"\\n\").str[1]\n",
    "df[\"text\"] = df[\"text\"].str.split(\"\\n\").apply(lambda x: ','.join(x[2:]))\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\n','')\n",
    "#have to split up to have less than 1600 tokens\n",
    "#this will split into sections (1) ... (2) ...\n",
    "df['split_text'] = df['text'].str.split(r'\\(\\d+\\)')\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\n','')\n",
    "df = df.explode(\"split_text\")\n",
    "df = df[df[\"split_text\"]!= \"\"]\n",
    "df[\"nr\"] = df[\"nr\"].str.replace('para','')\n",
    "df['token_count'] = df['split_text'].apply(num_tokens_from_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bb4c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>nr</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>split_text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VVS</td>\n",
       "      <td>1</td>\n",
       "      <td>(1) Vabariigi Valitsus teostab täidesaatvat ri...</td>\n",
       "      <td>https://www.riigiteataja.ee/akt/VVS#para1</td>\n",
       "      <td>Vabariigi Valitsuse pädevus</td>\n",
       "      <td>Vabariigi Valitsus teostab täidesaatvat riigi...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type nr                                               text  \\\n",
       "0  VVS  1  (1) Vabariigi Valitsus teostab täidesaatvat ri...   \n",
       "\n",
       "                                        link                        title  \\\n",
       "0  https://www.riigiteataja.ee/akt/VVS#para1  Vabariigi Valitsuse pädevus   \n",
       "\n",
       "                                          split_text  token_count  \n",
       "0   Vabariigi Valitsus teostab täidesaatvat riigi...           39  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821adc34",
   "metadata": {},
   "source": [
    "Now, we must separate paragraphs, which have more tokens than 1600 and perform additional cleaning. This code will split the longer paragraphs by sections if there are subsections of sections, like (3.1) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62e576a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "over_length = df[df[\"token_count\"]>1600]\n",
    "#remove the long strings for now\n",
    "df = df[df[\"token_count\"]<1600]\n",
    "over_length['split_text_2'] = over_length['split_text'].str.split(r'\\(\\d+\\.\\d+\\)')\n",
    "over_length = over_length.explode(\"split_text_2\")\n",
    "over_length['token_count'] = over_length['split_text_2'].apply(num_tokens_from_string)\n",
    "#if they still have too many words, most often the paragraphs are lists of definitions, which can be split by list enumeration\n",
    "#other with a shorter length can be added back to original dataframe\n",
    "over_length_merge = over_length[over_length[\"token_count\"]<1600]\n",
    "over_length_merge = over_length_merge.drop(columns=[\"split_text\"]).rename(columns={\"split_text_2\":\"split_text\"})\n",
    "df = pd.concat([df,over_length_merge])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58579111",
   "metadata": {},
   "source": [
    "This code is used in case there still be paragraphs with more tokens than 1600. This will split the paragraph by list enumeration elements, so 1) ..., 2) ... ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d356ecfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "over_length = over_length[over_length[\"token_count\"]>1600]\n",
    "over_length[\"title_2\"] = over_length[\"split_text_2\"].str.split(r',\\d+[\\.\\d+]*\\)').str[0]\n",
    "over_length[\"split_text_3\"] = over_length[\"split_text_2\"].str.split(r',\\d+[\\.\\d+]*\\)')\n",
    "over_length = over_length.explode(\"split_text_3\")\n",
    "over_length = over_length[over_length[\"title_2\"]!=over_length[\"split_text_3\"]]\n",
    "over_length['token_count'] = over_length['split_text_3'].apply(num_tokens_from_string)\n",
    "over_length[\"title\"] = over_length[\"title\"]+ '. ' + over_length[\"title_2\"]\n",
    "over_length = over_length.drop(columns=[\"split_text\",\"split_text_2\",\"title_2\"]).rename(columns={\"split_text_3\":\"split_text\"})\n",
    "df = pd.concat([df,over_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9108e98e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True) #some laws do not have the type\n",
    "df[\"concatenated\"] = \"Seadus \"+ df[\"type\"]+\" paragrahv \"+ df[\"nr\"]+\". Pealkiri: \"+ df[\"title\"]+ \" Sisu: \"+ df[\"split_text\"]\n",
    "df[\"concatenated\"] = df[\"concatenated\"].str.replace(r'\\s+', ' ',regex=True).str.rstrip(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac2a0c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seadus VVS paragrahv 1. Pealkiri: Vabariigi Valitsuse pädevus Sisu: Vabariigi Valitsus teostab täidesaatvat riigivõimu Eesti Vabariigi põhiseaduse ja seaduste alusel.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"concatenated\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7cae55",
   "metadata": {},
   "source": [
    "### Calculating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b062103",
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = np.array(df[\"concatenated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8681a1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seadus VVS paragrahv 1. Pealkiri: Vabariigi Valitsuse pädevus Sisu: Vabariigi Valitsus teostab täidesaatvat riigivõimu Eesti Vabariigi põhiseaduse ja seaduste alusel.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laws[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d870fc21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n",
      "Batch 1000 to 1999\n",
      "Batch 2000 to 2999\n",
      "Batch 3000 to 3999\n",
      "Batch 4000 to 4999\n",
      "Batch 5000 to 5999\n",
      "Batch 6000 to 6999\n",
      "Batch 7000 to 7999\n",
      "Batch 8000 to 8999\n",
      "Batch 9000 to 9999\n",
      "Batch 10000 to 10999\n",
      "Batch 11000 to 11999\n",
      "Batch 12000 to 12999\n",
      "Batch 13000 to 13999\n",
      "Batch 14000 to 14999\n",
      "Batch 15000 to 15999\n",
      "Batch 16000 to 16999\n",
      "Batch 17000 to 17999\n",
      "Batch 18000 to 18999\n",
      "Batch 19000 to 19999\n",
      "Batch 20000 to 20999\n",
      "Batch 21000 to 21999\n",
      "Batch 22000 to 22999\n",
      "Batch 23000 to 23999\n",
      "Batch 24000 to 24999\n",
      "Batch 25000 to 25999\n",
      "Batch 26000 to 26999\n",
      "Batch 27000 to 27999\n",
      "Batch 28000 to 28999\n",
      "Batch 29000 to 29999\n",
      "Batch 30000 to 30999\n",
      "Batch 31000 to 31999\n",
      "Batch 32000 to 32999\n",
      "Batch 33000 to 33999\n",
      "Batch 34000 to 34999\n",
      "Batch 35000 to 35999\n",
      "Batch 36000 to 36999\n",
      "Batch 37000 to 37999\n",
      "Batch 38000 to 38999\n",
      "Batch 39000 to 39999\n",
      "Batch 40000 to 40999\n",
      "Batch 41000 to 41999\n",
      "Batch 42000 to 42999\n",
      "Batch 43000 to 43999\n",
      "Batch 44000 to 44999\n",
      "Batch 45000 to 45999\n",
      "Batch 46000 to 46999\n",
      "Batch 47000 to 47999\n",
      "Batch 48000 to 48999\n",
      "Batch 49000 to 49999\n",
      "Batch 50000 to 50999\n",
      "Batch 51000 to 51999\n",
      "Batch 52000 to 52999\n",
      "Batch 53000 to 53999\n",
      "Batch 54000 to 54999\n",
      "Batch 55000 to 55999\n",
      "Batch 56000 to 56999\n",
      "Batch 57000 to 57999\n",
      "Batch 58000 to 58999\n",
      "Batch 59000 to 59999\n",
      "Batch 60000 to 60999\n",
      "Batch 61000 to 61999\n",
      "Batch 62000 to 62999\n",
      "Batch 63000 to 63999\n",
      "Batch 64000 to 64999\n",
      "Batch 65000 to 65999\n",
      "Batch 66000 to 66999\n",
      "Batch 67000 to 67999\n",
      "Batch 68000 to 68999\n",
      "Batch 69000 to 69999\n",
      "Batch 70000 to 70999\n",
      "Batch 71000 to 71999\n",
      "Batch 72000 to 72999\n",
      "Batch 73000 to 73999\n",
      "Batch 74000 to 74999\n",
      "Batch 75000 to 75999\n",
      "Batch 76000 to 76999\n",
      "Batch 77000 to 77999\n",
      "Batch 78000 to 78999\n",
      "Batch 79000 to 79999\n",
      "Batch 80000 to 80999\n",
      "Batch 81000 to 81999\n",
      "Batch 82000 to 82999\n",
      "Batch 83000 to 83999\n",
      "Batch 84000 to 84999\n",
      "Batch 85000 to 85999\n",
      "Batch 86000 to 86999\n",
      "Batch 87000 to 87999\n",
      "Batch 88000 to 88999\n",
      "Batch 89000 to 89999\n",
      "Batch 90000 to 90999\n",
      "Batch 91000 to 91999\n",
      "Batch 92000 to 92999\n",
      "Batch 93000 to 93999\n",
      "Batch 94000 to 94999\n",
      "Batch 95000 to 95999\n",
      "Batch 96000 to 96999\n",
      "Batch 97000 to 97999\n",
      "Batch 98000 to 98999\n",
      "Batch 99000 to 99999\n",
      "Batch 100000 to 100999\n",
      "Batch 101000 to 101999\n",
      "Batch 102000 to 102999\n",
      "Batch 103000 to 103999\n",
      "Batch 104000 to 104999\n",
      "Batch 105000 to 105999\n",
      "Batch 106000 to 106999\n",
      "Batch 107000 to 107999\n",
      "Batch 108000 to 108999\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "\n",
    "##\n",
    "##DO NOT RUN UNLESS NEED TO CREATE NEW EMBEDDINGS (THIS CODE COSTS ABT 2 DOLLARS)\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  \n",
    "\n",
    "law_strings = laws.tolist()\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(law_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = law_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "result = pd.DataFrame({\"text\": law_strings, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d9f77d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annilo\\AppData\\Local\\Temp\\ipykernel_15976\\1531630278.py:2: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['text', 'embedding'], dtype='object')]\n",
      "\n",
      "  result.to_hdf(r'embeddedfile_all.h5', key='stage', mode='w')\n"
     ]
    }
   ],
   "source": [
    "#loading is super slow\n",
    "result.to_hdf(r'embeddedfile_all.h5', key='stage', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e7e9a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = save_index(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93e52f",
   "metadata": {},
   "source": [
    "## Using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6683ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reread = pd.read_hdf('./embeddedfile_all.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25178f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e943a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06cdc7",
   "metadata": {},
   "source": [
    "The spatial distance cosine is calculated here. The cosine distance ranges from -1 to 1, where 1 indicates that the vectors are identical and -1 that they are completely dissimilar. When calculating relatedness, it is common to subtract the cosine distance from 1, to better reflect the relatedness from 0 to 2, where 2 would mean not related at all and 0 meaning perfect similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e53bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seadus TsMS paragrahv 113. Pealkiri: Lapsendamine Sisu: Lapsendamist käsitlev avaldus esitatakse lapsendatava elukoha järgi. Kui lapsendataval ei ole Eestis elukohta, esitatakse avaldus Harju Maakohtusse.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seadus TsMS paragrahv 113. Pealkiri: Lapsendamine Sisu: Lapsendamisasja võib lahendada Eesti kohus, kui lapsendaja, üks lapsendavatest abikaasadest või laps on Eesti Vabariigi kodanik või kui lapsendaja, ühe lapsendava abikaasa või lapse elukoht on Eestis.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seadus TsMS paragrahv 564. Pealkiri: Lapsendamise avaldus Sisu: Avaldaja märgib avalduses oma sünniaasta, -kuu ja -päeva, samuti asjaolud, mis kinnitavad, et ta on suuteline last kasvatama, tema eest hoolitsema ja teda ülal pidama.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seadus PKS paragrahv 158. Pealkiri: Lapsendamise ettevalmistamine Sisu: Kui Sotsiaalkindlustusamet seda nõuab, läbib lapsendada sooviv isik lapsendamisele eelnevalt asjakohase koolitusprogrammi.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seadus PKS paragrahv 147. Pealkiri: Lapsendamise lubatavus Sisu: Lapsendada on lubatud, kui see on lapse huvides vajalik ning on alust arvata, et lapsendaja ja lapse vahel tekib vanema ja lapse suhe. Lapsendajat valides arvestatakse tema isikuomadusi, suhteid lapsendatavaga, varalist seisundit ja võimet täita lapsendamissuhtest tulenevaid kohustusi, samuti võimaluse korral lapse vanemate eeldatavat tahet. Otsustamisel arvestatakse võimaluse korral ka lapse üleskasvatamise järjepidevuse vajadust ning tema rahvuslikku, usulist, kultuurilist ja keelelist päritolu.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strings, relatednesses = strings_ranked_by_relatedness_vector(\"Lapsendamine\",EMBEDDING_MODEL,index,df,openai)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "669f548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness_vector(query,EMBEDDING_MODEL, index,df,openai)\n",
    "    introduction = 'Use the following part of the law to answer the subsequent question. Try to find the best answer.' \\\n",
    "    'Formulate the answer including the law name and paragraph number'\\\n",
    "    'The answer should be a coherent sentence. If the answer cannot be found in the laws, write '\\\n",
    "    '\"Ei leidnud seadustest küsimusele vastust, proovige küsimus ümber sõnastada\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nSeaduse lõik:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Sa vastad Eesti seaduste andmebaasi küsimustele.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a144de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seaduse VVS paragrahv 6 kohaselt astub Vabariigi Valitsus või minister ametisse ametivande andmisega Riigikogu ees.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Kuidas astub minister ametisse?\"\n",
    "answer = ask(question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f931e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hinnang vastusele: I would rate this answer a 10 as it directly answers the question and provides a specific legal reference to support the answer.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_answer(question,answer,openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cdc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
